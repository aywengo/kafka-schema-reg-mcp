# Kafka Schema Registry MCP Server - Use Cases

This document outlines various real-world use cases and scenarios for the Kafka Schema Registry MCP Server v2.0.0 with **FastMCP 2.8.0+ Framework**, **MCP 2025-06-18 Specification Compliance**, **Enhanced Authentication**, **Claude Desktop Integration**, **Async Task Management**, and **Multi-Registry Support**.

## ü§ñ MCP Integration Overview

The Kafka Schema Registry MCP Server uses the **modern FastMCP 2.8.0+ framework** with full **MCP 2025-06-18 specification compliance**, integrating seamlessly with Claude Desktop and other MCP clients. Users interact with schema management through **natural language commands** with enhanced performance, reliability, and authentication capabilities.

### **Key MCP Features:**
- ‚úÖ **FastMCP 2.8.0+ Framework**: Modern MCP architecture with enhanced performance
- ‚úÖ **MCP 2025-06-18 Compliance**: Full support for latest Message Control Protocol specification
- ‚úÖ **Enhanced Authentication**: Native FastMCP BearerAuth with OAuth 2.0 support for 5 providers
- ‚úÖ **48 MCP Tools**: Complete schema operations via natural language  
- ‚úÖ **Claude Desktop Ready**: Direct AI integration for schema management
- ‚úÖ **Natural Language Interface**: No curl commands or API knowledge required
- ‚úÖ **Real-time Interaction**: Immediate feedback through Claude Desktop
- ‚úÖ **Context-Aware Operations**: All tools support schema contexts
- ‚úÖ **Export Capabilities**: JSON and Avro IDL formats with comprehensive metadata
- ‚úÖ **Async Task Management**: Non-blocking operations with progress tracking
- ‚úÖ **Multi-Registry Support**: Manage multiple Schema Registry instances

---

## üè¢ Enterprise Use Cases

### 1. Multi-Environment Schema Management with AI Assistant

#### **Scenario**: Large Enterprise with Multiple Deployment Environments
A technology company with microservices architecture needs to manage schemas across development, staging, and production environments using AI-assisted workflows.

**Claude Desktop Implementation:**
```

**Schema Development Workflow:**
```
Human: "Register a user event schema in the development context with fields for userId, eventType, and timestamp"

Claude: I'll register that user event schema in your development context.

[Uses register_schema MCP tool]
‚úÖ Schema registered: user-events (ID: 1) in development context
```

**Promotion Workflow:**
```
Human: "Check if the user-events schema from development is compatible with production, then promote it to staging if compatible"

Claude: I'll check compatibility first and then promote if safe.

[Uses check_compatibility MCP tool]
‚úÖ Compatibility check: Schema is BACKWARD compatible with production

[Uses register_schema MCP tool]
‚úÖ Schema promoted to staging context: user-events (ID: 2)
```

**MCP Benefits:**
- ‚úÖ **Natural Language**: No technical API knowledge required
- ‚úÖ **AI-Assisted**: Claude helps with complex workflows
- ‚úÖ **Environment Isolation**: Prevents accidental production impact
- ‚úÖ **Instant Feedback**: Real-time validation and results

---

### 2. Multi-Tenant SaaS Platform with Context Isolation

#### **Scenario**: SaaS Platform with Client-Specific Schema Requirements
A platform serving multiple clients where each client may have customized data structures and compliance requirements.

**Client Setup:**
```
Human: "Create separate contexts for our clients: ACME Corp, Globex Industries, and Initech"

Claude: I'll set up isolated contexts for each of your clients.

[Uses create_context MCP tool for each client]
‚úÖ Created context: client-acme  
‚úÖ Created context: client-globex
‚úÖ Created context: client-initech
```

**GDPR-Compliant Schema for EU Client:**
```
Human: "Register a user profile schema in the ACME context with GDPR compliance fields including consent and retention period"

Claude: I'll create a GDPR-compliant user profile schema for ACME.

[Uses register_schema MCP tool]
‚úÖ Registered schema: user-profile in client-acme context
   Fields: userId, email, gdprConsent, dataRetentionDays (default: 365)
```

**HIPAA-Compliant Schema for Healthcare Client:**
```
Human: "Register a similar user profile schema for Globex but with HIPAA compliance fields instead of GDPR"

Claude: I'll create a HIPAA-compliant version for Globex Industries.

[Uses register_schema MCP tool]
‚úÖ Registered schema: user-profile in client-globex context  
   Fields: userId, email, hipaaCompliant, encryptionLevel (default: AES256)
```

**MCP Benefits:**
- ‚úÖ **Complete Tenant Isolation**: Each client has their own schema space
- ‚úÖ **Compliance-Specific Schemas**: GDPR, HIPAA, PCI variations
- ‚úÖ **Natural Language Specification**: Describe compliance requirements in plain English
- ‚úÖ **Independent Evolution**: Clients can evolve schemas independently

---

## üîß Development Workflow Use Cases

### 3. Schema Evolution with AI-Assisted Compatibility Testing

#### **Scenario**: Safe Schema Evolution with Backward Compatibility
A development team needs to evolve their order schema while ensuring all dependent services remain functional.

**Current State Analysis:**
```
Human: "Show me the current order schema in production and its version history"

Claude: Let me check the current state of your order schema.

[Uses get_schema and get_schema_versions MCP tools]
üìã Current schema: order-events v3 in production
   Fields: orderId, customerId, amount, currency
üìä Version history: v1, v2, v3 (3 versions total)
```

**Evolution Design:**
```
Human: "I want to add optional fields for shipping address and order notes. Check if this evolution is backward compatible"

Claude: I'll design the evolution and test compatibility for you.

[Uses check_compatibility MCP tool]
‚úÖ Evolution design: Adding optional fields with defaults
‚úÖ Compatibility test: BACKWARD compatible - safe to deploy
   New fields: shippingAddress (optional), orderNotes (optional, default: null)
```

**Staged Rollout:**
```
Human: "Register the evolved schema in development first, then promote to staging if tests pass"

Claude: I'll implement a staged rollout for safety.

[Uses register_schema MCP tool]
‚úÖ Registered evolved schema: order-events v4 in development
üß™ Ready for testing in development environment
```

**MCP Benefits:**
- ‚úÖ **AI-Assisted Evolution**: Claude helps design backward-compatible changes
- ‚úÖ **Automated Compatibility Testing**: Instant validation before deployment
- ‚úÖ **Staged Rollout Support**: Safe promotion through environments
- ‚úÖ **Risk-Free Iteration**: Test in development without production impact

---

### 4. Feature Branch Schema Development

#### **Scenario**: Isolated Schema Development for New Features
Developers working on a new user preferences feature need isolated schema development without affecting the main development branch.

**Feature Branch Setup:**
```
Human: "Create a feature context for the new user preferences feature we're developing"

Claude: I'll create an isolated context for your user preferences feature.

[Uses create_context MCP tool]
‚úÖ Created context: feature-user-preferences
```

**Feature Schema Development:**
```
Human: "Register a schema for user preference updates with userId, preferences map, and timestamp"

Claude: I'll create the user preferences schema in your feature context.

[Uses register_schema MCP tool]
‚úÖ Registered schema: user-preferences-updated in feature-user-preferences context
   Fields: userId (string), preferences (map<string>), updatedAt (long)
```

**Integration Testing:**
```
Human: "Show me all schemas in the feature context and check if they're compatible with main development"

Claude: I'll review your feature schemas and test compatibility.

[Uses list_subjects and check_compatibility MCP tools]
üìã Feature schemas: user-preferences-updated
‚úÖ Compatibility with development: FORWARD compatible - safe to merge
```

**Cleanup After Merge:**
```
Human: "The feature has been merged. Clean up the feature context"

Claude: I'll clean up the feature context for you.

[Uses delete_context MCP tool]
‚úÖ Deleted context: feature-user-preferences
‚ôªÔ∏è Resources cleaned up
```

**MCP Benefits:**
- ‚úÖ **Isolated Development**: No conflicts with main development
- ‚úÖ **Easy Cleanup**: Simple context deletion after merge
- ‚úÖ **Parallel Feature Work**: Multiple teams can work independently
- ‚úÖ **Integration Validation**: Test compatibility before merging

---

## üìä Export & Documentation Use Cases

### 5. Comprehensive Schema Documentation Generation

#### **Scenario**: Documentation Team Needs Human-Readable Schema Documentation
A documentation team needs to generate comprehensive, human-readable documentation for all schemas across different environments for developer onboarding and API documentation.

**Documentation Export:**
```
Human: "Export all production schemas in Avro IDL format for our API documentation"

Claude: I'll export your production schemas in human-readable Avro IDL format.

[Uses export_context MCP tool]
‚úÖ Exported production context: 15 schemas in Avro IDL format
üìã Schemas include: user-events, order-events, payment-events, etc.
üîó Generated beautiful documentation with field descriptions
```

**Example Generated Documentation:**
```idl
@namespace("com.company.events")
/** User event tracking schema */  
record UserEvent {
  /** Unique user identifier */
  string userId;
  /** Type of event performed */
  string eventType;
  /** Event timestamp in milliseconds */
  long timestamp;
  /** Optional event metadata */
  map<string> metadata = {};
}
```

**Version History Export:**
```
Human: "Export the complete version history for the user-events schema including all metadata"

Claude: I'll export the complete history of your user-events schema.

[Uses export_subject MCP tool]
‚úÖ Exported user-events: All 4 versions with complete metadata
üìä Version progression: v1 ‚Üí v2 (added email) ‚Üí v3 (added metadata) ‚Üí v4 (current)
üìã Includes: compatibility levels, registration timestamps, evolution notes
```

**MCP Benefits:**
- ‚úÖ **Human-Readable Output**: Beautiful Avro IDL with documentation
- ‚úÖ **Complete History**: All versions and evolution metadata
- ‚úÖ **AI-Enhanced Documentation**: Claude adds context and explanations
- ‚úÖ **Multiple Formats**: JSON for tooling, IDL for humans

---

### 6. Compliance and Audit Export

#### **Scenario**: Financial Institution Needs Regulatory Compliance Exports
A financial institution must generate regular exports for regulatory compliance, including GDPR data lineage and SOX audit requirements.

**GDPR Compliance Export:**
```
Human: "Export all schemas from our GDPR context with complete metadata for our quarterly compliance report"

Claude: I'll generate a comprehensive GDPR compliance export for you.

[Uses export_context MCP tool]
‚úÖ Exported GDPR context: 8 schemas with full compliance metadata
üìã Included: data retention policies, consent tracking, subject rights
üìÖ Export timestamp: Ready for Q4 2024 compliance submission
```

**SOX Audit Trail:**
```
Human: "Generate an audit trail showing all schema changes in the financial context for the past quarter"

Claude: I'll create a complete audit trail for your SOX compliance.

[Uses export_context with version history]
‚úÖ Generated audit trail: financial context
üìä Schema changes: 12 modifications across 6 schemas
üîç Includes: who, what, when, compatibility impacts
üìã SOX-compliant format with digital signatures
```

**PCI DSS Security Review:**
```
Human: "Show me all payment-related schemas and verify they don't contain sensitive cardholder data"

Claude: I'll review your payment schemas for PCI compliance.

[Uses list_subjects and get_schema MCP tools]
‚úÖ Reviewed payment schemas: 3 schemas analyzed
üîí PCI Compliance: All schemas use tokenized card references only
‚ö†Ô∏è No sensitive cardholder data found in schema definitions
üìã Ready for PCI assessment
```

**MCP Benefits:**
- ‚úÖ **Regulatory-Ready Exports**: Formatted for compliance submissions
- ‚úÖ **Complete Audit Trails**: Who changed what and when
- ‚úÖ **Security Validation**: AI-assisted compliance checking
- ‚úÖ **Automated Reporting**: Scheduled compliance exports

---

## üéõÔ∏è Advanced Configuration Use Cases

### 7. Dynamic Configuration Management

#### **Scenario**: Operations Team Needs to Manage Compatibility and Mode Settings
A platform operations team needs to dynamically adjust schema compatibility requirements and operational modes across different environments.

**Environment-Specific Configuration:**
```
Human: "Set the production context to FULL compatibility mode for maximum safety, but keep development at BACKWARD for flexibility"

Claude: I'll configure different compatibility levels for your environments.

[Uses update_global_config MCP tool]
‚úÖ Production context: Set to FULL compatibility (maximum safety)
‚úÖ Development context: Set to BACKWARD compatibility (flexibility)
üìã Configuration applied to all subjects in respective contexts
```

**Mode Management:**
```
Human: "Switch the schema registry to VIEWONLY mode during our maintenance window"

Claude: I'll switch to read-only mode for your maintenance.

[Uses update_mode MCP tool]
‚úÖ Global mode: Changed to VIEWONLY
üö´ Schema registration temporarily disabled
‚è∞ Ready for maintenance operations
```

**Subject-Specific Overrides:**
```
Human: "Set stricter FULL compatibility for our critical payment schemas but keep BACKWARD for internal events"

Claude: I'll apply subject-specific compatibility settings.

[Uses update_subject_config MCP tool]
‚úÖ payment-events: Set to FULL compatibility
‚úÖ payment-processed: Set to FULL compatibility  
‚úÖ internal-events: Kept at BACKWARD compatibility
üîí Critical payments now have maximum safety
```

**MCP Benefits:**
- ‚úÖ **Dynamic Configuration**: Real-time adjustments without restarts
- ‚úÖ **Context-Aware Settings**: Different rules per environment
- ‚úÖ **Subject-Level Control**: Fine-grained compatibility management
- ‚úÖ **Natural Language Commands**: No complex configuration syntax

---

## üöÄ Real-World Workflow Examples

### 8. Complete Development Lifecycle

#### **Scenario**: End-to-End Schema Development Workflow
A complete example showing how a team develops, tests, and deploys a new schema using the MCP server.

**Day 1: Initial Development**
```
Human: "We're starting a new feature for order tracking. Create a feature context and register an initial schema with orderId, status, and timestamp"

Claude: I'll set up your order tracking feature development.

[Uses create_context and register_schema MCP tools]
‚úÖ Created context: feature-order-tracking
‚úÖ Registered schema: order-status-updated v1
   Fields: orderId (string), status (enum), timestamp (long)
```

**Day 3: Schema Evolution**
```
Human: "We need to add customer notification preferences and estimated delivery date to the order status schema. Check compatibility first"

Claude: I'll evolve your schema and validate compatibility.

[Uses check_compatibility and register_schema MCP tools]
‚úÖ Compatibility check: BACKWARD compatible evolution
‚úÖ Registered schema: order-status-updated v2
   Added: notificationPrefs (optional), estimatedDelivery (optional)
```

**Day 5: Integration Testing**
```
Human: "Move the order tracking schema to our integration testing context and run compatibility tests against all related schemas"

Claude: I'll set up integration testing for your schema.

[Uses register_schema and check_compatibility MCP tools]
‚úÖ Schema registered in integration context
‚úÖ Compatibility with: order-events (‚úì), customer-notifications (‚úì), delivery-events (‚úì)
üß™ Ready for integration testing
```

**Day 8: Production Deployment**
```
Human: "All tests passed! Promote the order tracking schema to staging, then production after staging validation"

Claude: I'll execute the production deployment workflow.

[Uses register_schema MCP tool for staged deployment]
‚úÖ Promoted to staging: order-status-updated v2
‚è≥ Waiting for staging validation...
‚úÖ Promoted to production: order-status-updated v2
üéâ Feature successfully deployed!
```

**Cleanup:**
```
Human: "Clean up the feature and integration contexts since we're done"

Claude: I'll clean up the temporary development contexts.

[Uses delete_context MCP tool]
‚úÖ Deleted context: feature-order-tracking
‚úÖ Deleted context: integration-testing
‚ôªÔ∏è Development resources cleaned up
```

**MCP Benefits:**
- ‚úÖ **Complete Workflow Support**: From development to production
- ‚úÖ **AI-Guided Process**: Claude helps with best practices
- ‚úÖ **Safety Validation**: Compatibility checks at every step
- ‚úÖ **Automated Cleanup**: No orphaned development resources

---

## üöÄ Async Operations Use Cases

### 9. Large-Scale Schema Migration with Progress Tracking

#### **Scenario**: Migrating Hundreds of Schemas Between Environments
A company needs to migrate their entire schema registry from staging to production, containing over 200 schemas across multiple contexts.

**Initiating Large Migration:**
```
Human: "Migrate all schemas from the staging registry to production"

Claude: I'll generate the Docker migration command for staging to production.

[Uses migrate_context MCP tool]
üöÄ Ready to migrate staging registry to production:

Copy and run this command:
docker run --platform linux/amd64 --network host -it --rm \
  -e SOURCE_SCHEMA_REGISTRY_URL=http://staging-registry:8081 \
  -e DEST_SCHEMA_REGISTRY_URL=http://prod-registry:8082 \
  -e SOURCE_USERNAME=staging_user \
  -e SOURCE_PASSWORD=staging_pass \
  -e DEST_USERNAME=prod_user \
  -e DEST_PASSWORD=prod_pass \
  -e ENABLE_MIGRATION=true \
  -e DRY_RUN=true \
  -e PRESERVE_IDS=true \
  -e DEST_IMPORT_MODE=true \
  aywengo/kafka-schema-reg-migrator:latest

Migration scope: All contexts from staging registry (215 schemas)
Set DRY_RUN=false when ready for actual migration.

‚ö†Ô∏è This is a DRY RUN - no actual changes will be made
‚ö†Ô∏è Ensure Docker is installed and running
‚ö†Ô∏è Large migration - monitor output for progress
```

**Monitoring Migration Progress:**
```
Human: "How can I monitor the migration once it's running?"

Claude: The external migrator provides comprehensive real-time output:

# The Docker command shows live progress:
- Current context being migrated
- Schemas processed: 87/215
- Failed schemas with retry count
- Estimated time remaining

# Example output:
2024-06-07 16:49:15 - INFO - Processing subject: user-events
2024-06-07 16:49:15 - INFO - [DRY RUN] Would migrate user-events version 1 - compatible
2024-06-07 16:49:15 - INFO - Progress: 87/215 schemas (40% complete)

For individual schema migrations, use migrate_schema which returns a task ID for monitoring with get_migration_progress.
```

**MCP Benefits:**
- ‚úÖ **Non-Blocking Operations**: Start migrations and continue working
- ‚úÖ **Real-Time Progress**: Monitor large operations without timeout
- ‚úÖ **Automatic Retry Logic**: Handles transient failures
- ‚úÖ **Detailed Progress Tracking**: Know exactly what's happening

---

### 10. Multi-Registry Schema Synchronization

#### **Scenario**: Global Company with Regional Schema Registries
A multinational company operates Schema Registries in different regions (US, EU, APAC) and needs to synchronize schemas while respecting data residency requirements.

**Registry Health Check:**
```
Human: "Check the status of all our regional schema registries"

Claude: I'll check the health of all your regional registries.

[Uses test_all_registries MCP tool]
üè• Registry Health Check:
   ‚úÖ us-east: Connected (45ms) - Primary US registry
   ‚úÖ eu-central: Connected (120ms) - EU GDPR-compliant registry
   ‚úÖ apac-tokyo: Connected (180ms) - APAC regional registry
   ‚ö†Ô∏è us-west: Degraded (850ms) - High latency detected
   
   Overall: 3/4 healthy, 1 degraded
```

**Cross-Registry Comparison:**
```
Human: "Compare the customer schemas between US and EU registries to ensure GDPR compliance"

Claude: I'll start comparing customer schemas across regions for compliance.

[Uses compare_contexts_across_registries MCP tool]
üîÑ Comparison started:
   Task ID: comp-20240531-us-eu-customer
   Comparing: US/customer ‚Üî EU/customer
   Focus: GDPR compliance fields
   Status: Running (15% - Analyzing field differences...)
```

**Selective Schema Promotion:**
```
Human: "Promote only the GDPR-compliant schemas from US to EU registry"

Claude: I'll identify and migrate only GDPR-compliant schemas.

[Uses migrate_schema MCP tool with filtering]
üîÑ Selective migration started:
   Task ID: mig-20240531-gdpr-selective
   Source: us-east/customer
   Target: eu-central/customer
   Filter: GDPR-compliant schemas only
   
   Identified schemas:
   ‚úÖ customer-profile-v3 (has consent fields)
   ‚úÖ customer-preferences-v2 (has retention policy)
   ‚ùå customer-analytics-v1 (missing GDPR fields)
   
   Migrating: 2/3 schemas
```

**MCP Benefits:**
- ‚úÖ **Multi-Region Support**: Manage global infrastructure
- ‚úÖ **Compliance-Aware Migration**: Filter based on requirements
- ‚úÖ **Cross-Registry Comparison**: Identify differences
- ‚úÖ **Selective Synchronization**: Migrate only what's needed

---

### 11. Batch Environment Cleanup

#### **Scenario**: Cleaning Up After Feature Development Sprint
After a two-week sprint, the team has created multiple feature branch contexts that need to be cleaned up efficiently.

**Identify Cleanup Candidates:**
```
Human: "Show me all contexts that start with 'feature-' so we can clean them up"

Claude: I'll identify all feature branch contexts for cleanup.

[Uses list_contexts MCP tool]
üìã Feature contexts found:
   ‚Ä¢ feature-user-dashboard (12 schemas)
   ‚Ä¢ feature-payment-v2 (8 schemas)
   ‚Ä¢ feature-mobile-api (15 schemas)
   ‚Ä¢ feature-analytics-beta (6 schemas)
   ‚Ä¢ feature-notifications (4 schemas)
   
   Total: 5 contexts with 45 schemas
```

**Batch Cleanup Preview:**
```
Human: "Clean up all those feature contexts, but show me what will be deleted first"

Claude: I'll preview the batch cleanup operation for you.

[Uses clear_multiple_contexts_batch MCP tool with dry_run=true]
üîç Batch Cleanup Preview:
   Task ID: clean-20240531-features (DRY RUN)
   
   Will delete:
   üìÅ feature-user-dashboard:
      ‚Ä¢ user-dashboard-events
      ‚Ä¢ dashboard-preferences
      ‚Ä¢ dashboard-widgets
      ... and 9 more schemas
   
   üìÅ feature-payment-v2:
      ‚Ä¢ payment-flow-v2
      ‚Ä¢ payment-confirmation-v2
      ... and 6 more schemas
   
   Total: 5 contexts, 45 schemas
   Estimated time: ~15 seconds
   
   No changes made (dry run mode)
```

**Execute Parallel Cleanup:**
```
Human: "Looks good, go ahead and clean them all up"

Claude: I'll execute the batch cleanup of all feature contexts.

[Uses clear_multiple_contexts_batch MCP tool with dry_run=false]
üßπ Batch cleanup started:
   Task ID: clean-20240531-features
   Mode: Parallel deletion (10 concurrent operations)
   Status: Running
   
   Progress:
   ‚úÖ feature-notifications: Completed (4/4 schemas deleted)
   ‚úÖ feature-payment-v2: Completed (8/8 schemas deleted) 
   üîÑ feature-user-dashboard: In progress (7/12 schemas deleted)
   ‚è≥ feature-mobile-api: Pending
   ‚è≥ feature-analytics-beta: Pending
```

**Monitor Cleanup Progress:**
```
Human: "How's the cleanup going?"

Claude: Let me check the cleanup progress.

[Uses get_cleanup_progress MCP tool]
üìä Cleanup Progress: clean-20240531-features
   Overall: 78% complete
   Duration: 8 seconds elapsed
   
   Completed: 3/5 contexts
   Schemas deleted: 35/45
   Current: Deleting feature-mobile-api schemas
   Rate: 4.4 schemas/second
   
   Estimated completion: ~3 seconds
```

**MCP Benefits:**
- ‚úÖ **Parallel Execution**: Fast cleanup with concurrent operations
- ‚úÖ **Safe Preview Mode**: Review before deleting
- ‚úÖ **Progress Tracking**: Monitor large cleanup operations
- ‚úÖ **Efficient Resource Management**: Clean up test data quickly

---

### 12. Disaster Recovery with Progress Monitoring

#### **Scenario**: Emergency Schema Registry Recovery
A production incident requires restoring schemas from a backup registry to a new instance.

**Initiate Emergency Recovery:**
```
Human: "We need to restore all production schemas from our backup registry to the new disaster recovery instance ASAP"

Claude: I'll generate an emergency recovery command from your backup registry.

[Uses migrate_context MCP tool for disaster recovery]
üö® Emergency Recovery Command Generated:

Copy and run this command for immediate recovery:
docker run --platform linux/amd64 --network host -it --rm \
  -e SOURCE_SCHEMA_REGISTRY_URL=http://backup-registry:8081 \
  -e DEST_SCHEMA_REGISTRY_URL=http://dr-registry:8082 \
  -e SOURCE_USERNAME=backup_user \
  -e SOURCE_PASSWORD=backup_pass \
  -e DEST_USERNAME=dr_user \
  -e DEST_PASSWORD=dr_pass \
  -e ENABLE_MIGRATION=true \
  -e DRY_RUN=false \
  -e PRESERVE_IDS=true \
  -e DEST_IMPORT_MODE=true \
  aywengo/kafka-schema-reg-migrator:latest

Recovery scope: All contexts from backup registry (127 schemas)

‚ö†Ô∏è This is a LIVE MIGRATION - changes will be made immediately
‚ö†Ô∏è Monitor output for progress and any errors
‚ö†Ô∏è External tool provides automatic retry and parallel processing
```

**Real-Time Recovery Monitoring:**
```
Human: "How do I track the recovery progress?"

Claude: Monitor the external migrator's real-time output:

# The Docker command shows live recovery progress:
[INFO] Starting disaster recovery...
[INFO] Source: backup-registry/production
[INFO] Target: dr-registry/production
[INFO] Processing context: payment-events (15 schemas)
[INFO] ‚úì payment-created (v1-5) migrated
[INFO] ‚úì payment-processed (v1-3) migrated
[INFO] Progress: 45/127 schemas (35%)
[INFO] Estimated time remaining: 1m 30s

The external migrator provides automatic retry and detailed error reporting for mission-critical recovery operations.

**Post-Recovery Validation:**
```
Human: "Verify that the disaster recovery registry matches our backup"

Claude: I'll verify the recovery was successful.

[Uses compare_registries MCP tool]
‚úÖ Recovery Validation Complete:
   Schemas matched: 127/127
   Versions matched: 100%
   Configurations matched: 100%
   
   Disaster recovery successful!
```

**MCP Benefits:**
- ‚úÖ **Emergency Response**: Fast disaster recovery
- ‚úÖ **Parallel Migration**: Maximum speed when needed
- ‚úÖ **Continuous Monitoring**: Real-time progress updates
- ‚úÖ **Validation Tools**: Ensure complete recovery

## üéØ Advanced Async Patterns

### 13. Scheduled Maintenance with Task Management

#### **Scenario**: Planned Maintenance Window Operations
Operations team needs to perform multiple maintenance tasks during a scheduled window.

**Queue Multiple Operations:**
```
Human: "During tonight's maintenance window, I need to: 1) Clean up all test contexts, 2) Migrate staging to production, 3) Update all compatibility settings to FULL"

Claude: I'll queue up all your maintenance operations.

[Uses multiple async tools]
üìã Maintenance Tasks Queued:
   1. Task ID: clean-maint-001 - Cleanup test contexts
   2. Task ID: mig-maint-002 - Staging‚ÜíProduction migration  
   3. Task ID: config-maint-003 - Update compatibility settings
   
   Total estimated time: 15-20 minutes
   Status: Queued (will start in sequence)
```

**Monitor All Operations:**
```
Human: "Show me the status of all maintenance tasks"

Claude: I'll display all your maintenance operations.

[Uses list_all_active_tasks MCP tool]
üìä Maintenance Window Status:
   
   ‚úÖ clean-maint-001: Completed (2m 15s)
      - Deleted 8 test contexts
      - Removed 67 test schemas
   
   üîÑ mig-maint-002: Running (67% - 5m elapsed)
      - Migrating production schemas
      - 89/132 schemas complete
   
   ‚è≥ config-maint-003: Queued
      - Will start after migration
      - Estimated duration: 30s
   
   System load: Moderate
   Expected completion: 11:45 PM
```

**MCP Benefits:**
- ‚úÖ **Task Orchestration**: Queue and monitor multiple operations
- ‚úÖ **Maintenance Mode**: Coordinate complex workflows
- ‚úÖ **Progress Dashboard**: Single view of all operations
- ‚úÖ **Time Estimation**: Plan maintenance windows accurately

## Schema Statistics and Monitoring

### Registry Health Monitoring
Monitor the health and growth of your Schema Registry instances:

```python
# Get comprehensive registry statistics
stats = get_registry_statistics(registry="production")
print(f"Production Registry Status:")
print(f"- Total Contexts: {stats['total_contexts']}")
print(f"- Total Schemas: {stats['total_schemas']}")
print(f"- Total Versions: {stats['total_versions']}")
print(f"- Avg Versions/Schema: {stats['avg_versions_per_schema']:.2f}")

# Monitor context growth
for context in stats['contexts']:
    print(f"\nContext: {context['name']}")
    print(f"- Schemas: {context['total_schemas']}")
    print(f"- Versions: {context['total_versions']}")
```

### Schema Evolution Tracking
Track schema evolution across different contexts:

```python
# Compare schema versions across contexts
dev_versions = count_schema_versions(
    subject="user-value",
    context="development"
)
prod_versions = count_schema_versions(
    subject="user-value",
    context="production"
)

print(f"Development: {dev_versions['total_versions']} versions")
print(f"Production: {prod_versions['total_versions']} versions")
print(f"Version difference: {abs(dev_versions['total_versions'] - prod_versions['total_versions'])}")
```

### Context Management
Monitor and manage contexts effectively:

```python
# Get context overview
contexts = count_contexts(registry="staging")
print(f"Staging Registry Contexts:")
for context in contexts['contexts']:
    schemas = count_schemas(context=context, registry="staging")
    print(f"\n{context}:")
    print(f"- Total Schemas: {schemas['total_schemas']}")
    print(f"- Schema List: {', '.join(schemas['schemas'])}")
```

### Migration Planning
Use statistics to plan and validate migrations:

```python
# Pre-migration analysis
source_stats = get_registry_statistics(registry="development")
target_stats = get_registry_statistics(registry="production")

print("Migration Planning:")
print(f"Source (Development):")
print(f"- Contexts: {source_stats['total_contexts']}")
print(f"- Schemas: {source_stats['total_schemas']}")
print(f"- Versions: {source_stats['total_versions']}")

print(f"\nTarget (Production):")
print(f"- Contexts: {target_stats['total_contexts']}")
print(f"- Schemas: {target_stats['total_schemas']}")
print(f"- Versions: {target_stats['total_versions']}")

# Calculate migration scope
contexts_to_migrate = set(source_stats['contexts']) - set(target_stats['contexts'])
print(f"\nContexts to migrate: {len(contexts_to_migrate)}")
```

### Schema Cleanup
Identify unused or outdated schemas:

```python
# Analyze schema versions
for context in get_registry_statistics()['contexts']:
    print(f"\nAnalyzing {context['name']}:")
    for schema in context['schemas']:
        versions = count_schema_versions(
            subject=schema,
            context=context['name']
        )
        if versions['total_versions'] > 5:
            print(f"‚ö†Ô∏è  {schema}: {versions['total_versions']} versions")
            print(f"   Consider cleanup: {versions['versions']}")
```

### Multi-Registry Comparison
Compare statistics across multiple registries:

```python
# Compare registry statistics
registries = ["development", "staging", "production"]
for registry in registries:
    stats = get_registry_statistics(registry=registry)
    print(f"\n{registry.upper()} Registry:")
    print(f"- Contexts: {stats['total_contexts']}")
    print(f"- Schemas: {stats['total_schemas']}")
    print(f"- Versions: {stats['total_versions']}")
    print(f"- Avg Versions/Schema: {stats['avg_versions_per_schema']:.2f}")
```

### Best Practices
1. **Regular Monitoring**
   - Schedule regular statistics collection
   - Track growth trends
   - Set up alerts for unusual patterns

2. **Migration Planning**
   - Use statistics for migration scope
   - Validate target registry capacity
   - Track migration progress

3. **Cleanup Management**
   - Identify unused schemas
   - Track version history
   - Plan cleanup operations

4. **Performance Optimization**
   - Monitor schema distribution
   - Identify hot spots
   - Optimize context usage

5. **Security and Compliance**
   - Track schema changes
   - Monitor version history
   - Maintain audit trails

---

This comprehensive use cases document demonstrates how the Kafka Schema Registry MCP Server v1.7.0 transforms schema management with async operations, multi-registry support, and intelligent progress tracking. The MCP implementation enables teams to handle large-scale operations efficiently while Claude provides real-time monitoring and guidance throughout the process.